{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-03T13:52:42.013801Z","iopub.status.busy":"2022-05-03T13:52:42.013489Z","iopub.status.idle":"2022-05-03T13:52:42.043272Z","shell.execute_reply":"2022-05-03T13:52:42.042615Z","shell.execute_reply.started":"2022-05-03T13:52:42.013714Z"},"trusted":true},"outputs":[],"source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","import shutil\n","from pathlib import Path\n","\n","# Set transformer directory\n","transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","# Set input directory\n","input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n","\n","# Set convert file\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","\n","# Set conversion path\n","conversion_path = transformers_path/convert_file.name\n","\n","# If conversion path exists, delete it\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","# Copy convert file to transformers path\n","shutil.copy(convert_file, transformers_path)\n","\n","# Set DeBERTa v2 path\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","# For each filename in the list, copy the file to deberta_v2_path\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    \n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(input_dir/filename, filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:42.045572Z","iopub.status.busy":"2022-05-03T13:52:42.045296Z","iopub.status.idle":"2022-05-03T13:52:43.953616Z","shell.execute_reply":"2022-05-03T13:52:43.952802Z","shell.execute_reply.started":"2022-05-03T13:52:42.045536Z"},"trusted":true},"outputs":[],"source":["import os\n","import re\n","import gc\n","import ast\n","import sys\n","import copy\n","import json\n","import math\n","import string\n","import pickle\n","import random\n","import itertools\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","import tokenizers\n","import transformers\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","%env TOKENIZERS_PARALLELISM=true\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:43.955624Z","iopub.status.busy":"2022-05-03T13:52:43.955139Z","iopub.status.idle":"2022-05-03T13:52:43.965214Z","shell.execute_reply":"2022-05-03T13:52:43.964479Z","shell.execute_reply.started":"2022-05-03T13:52:43.955585Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed=42):\n","    '''\n","    Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.\n","    '''\n","    random.seed(seed)\n","    # Set a fixed value for the seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    \n","    # Apply seed to GPU\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        # When running on the CuDNN backend, two further options must be set\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","# Set seed for reproducibility\n","seed_everything(seed=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:43.970325Z","iopub.status.busy":"2022-05-03T13:52:43.969629Z","iopub.status.idle":"2022-05-03T13:52:43.981913Z","shell.execute_reply":"2022-05-03T13:52:43.981219Z","shell.execute_reply.started":"2022-05-03T13:52:43.970284Z"},"trusted":true},"outputs":[],"source":["def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    \n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","        \n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","        \n","    return micro_f1(bin_preds, bin_truths)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:43.987058Z","iopub.status.busy":"2022-05-03T13:52:43.985132Z","iopub.status.idle":"2022-05-03T13:52:44.002609Z","shell.execute_reply":"2022-05-03T13:52:44.001893Z","shell.execute_reply.started":"2022-05-03T13:52:43.987023Z"},"trusted":true},"outputs":[],"source":["def create_labels_for_scoring(df):\n","    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n","    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n","    for i in range(len(df)):\n","        # Get the location list for the current row\n","        lst = df.loc[i, 'location']\n","        if lst:\n","            # Join the location list with semicolons\n","            new_lst = ';'.join(lst)\n","            # Convert the new list to a literal eval\n","            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n","    # create labels\n","    truths = []\n","    # Iterate over the location list for each row\n","    for location_list in df['location_for_create_labels'].values:\n","        truth = []\n","        # If the location list is not empty\n","        if len(location_list) > 0:\n","            # Get the first location\n","            location = location_list[0]\n","            # Iterate over the locations in the location string\n","            for loc in [s.split() for s in location.split(';')]:\n","                # Convert the location string to a list of integers\n","                start, end = int(loc[0]), int(loc[1])\n","                # Append the start and end as a list to the truth list\n","                truth.append([start, end])\n","        # Append the truth list to the truths list\n","        truths.append(truth)\n","        \n","    return truths\n","\n","\n","def get_char_probs(texts, predictions, tokenizer):\n","    # Initialize results as a list of zeros with the length of each text\n","    results = [np.zeros(len(t)) for t in texts]\n","    # Iterate over the texts and predictions\n","    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n","        # Encode the text with special tokens and return offset mappings\n","        encoded = tokenizer(text, \n","                            add_special_tokens=True,\n","                            return_offsets_mapping=True)\n","        # Iterate over the offset mappings and predictions\n","        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n","            # Get the start and end of the current token\n","            start = offset_mapping[0]\n","            end = offset_mapping[1]\n","            # Set the prediction value for the current token\n","            results[i][start:end] = pred\n","            \n","    return results\n","\n","\n","def get_results(char_probs, th=0.5):\n","    # Initialize results as an empty list\n","    results = []\n","    # Iterate over the character probabilities\n","    for char_prob in char_probs:\n","        # Get the indices where the character probability is greater than or equal to the threshold\n","        result = np.where(char_prob >= th)[0] + 1\n","        # Group consecutive indices\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        # Format the result as a string of start and end indices\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        # Join the result with semicolons\n","        result = \";\".join(result)\n","        results.append(result)\n","        \n","    return results\n","\n","\n","def get_predictions(results):\n","    # Initialize predictions as an empty list\n","    predictions = []\n","    # Iterate over the results\n","    for result in results:\n","        prediction = []\n","        # If the result is not empty\n","        if result != \"\":\n","            # Iterate over the locations in the result string\n","            for loc in [s.split() for s in result.split(';')]:\n","                # Convert the location string to a list of integers\n","                start, end = int(loc[0]), int(loc[1])\n","                # Append the start and end as a list to the prediction list\n","                prediction.append([start, end])\n","        # Append the prediction list to the predictions list\n","        predictions.append(prediction)\n","        \n","    return predictions\n","\n","\n","def get_score(y_true, y_pred):\n","    # Calculate the micro f1 score for the true and predicted spans\n","    return span_micro_f1(y_true, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:44.00476Z","iopub.status.busy":"2022-05-03T13:52:44.004142Z","iopub.status.idle":"2022-05-03T13:52:44.014479Z","shell.execute_reply":"2022-05-03T13:52:44.013739Z","shell.execute_reply.started":"2022-05-03T13:52:44.004723Z"},"trusted":true},"outputs":[],"source":["def process_feature_text(text):\n","    # Replace 'I-year' with '1-year'\n","    text = re.sub('I-year', '1-year', text)\n","    # Replace '-OR-' with ' or '\n","    text = re.sub('-OR-', \" or \", text)\n","    # Replace '-' with ' '\n","    text = re.sub('-', ' ', text)\n","    return text\n","\n","\n","def clean_spaces(txt):\n","    # Replace newlines with spaces\n","    txt = re.sub('\\n', ' ', txt)\n","    # Replace tabs with spaces\n","    txt = re.sub('\\t', ' ', txt)\n","    # Replace carriage returns with spaces\n","    txt = re.sub('\\r', ' ', txt)\n","    # Replace multiple spaces with a single space\n","    txt = re.sub(r'\\s+', ' ', txt)\n","    return txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:44.018405Z","iopub.status.busy":"2022-05-03T13:52:44.017542Z","iopub.status.idle":"2022-05-03T13:52:44.703423Z","shell.execute_reply":"2022-05-03T13:52:44.702589Z","shell.execute_reply.started":"2022-05-03T13:52:44.018347Z"},"trusted":true},"outputs":[],"source":["# Set main directory\n","main_dir=\"../input/nbme-score-clinical-patient-notes/\"\n","\n","def preprocess_features(features):\n","    # Replace the feature text for the 27th row\n","    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n","\n","    return features\n","\n","\n","# Read test, submission, features, and patient notes\n","test = pd.read_csv(main_dir+'test.csv')\n","submission = pd.read_csv(main_dir+'sample_submission.csv')\n","features = pd.read_csv(main_dir+'features.csv')\n","patient_notes = pd.read_csv(main_dir+'patient_notes.csv')\n","\n","# Preprocess features\n","features = preprocess_features(features)\n","\n","# Print shapes and display head of each dataframe\n","print(f\"test.shape: {test.shape}\")\n","display(test.head())\n","print(f\"features.shape: {features.shape}\")\n","display(features.head())\n","print(f\"patient_notes.shape: {patient_notes.shape}\")\n","display(patient_notes.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:44.705076Z","iopub.status.busy":"2022-05-03T13:52:44.704737Z","iopub.status.idle":"2022-05-03T13:52:44.73239Z","shell.execute_reply":"2022-05-03T13:52:44.731558Z","shell.execute_reply.started":"2022-05-03T13:52:44.705038Z"},"trusted":true},"outputs":[],"source":["# Merge test with features on 'feature_num' and 'case_num'\n","test = test.merge(features, on=['feature_num', 'case_num'], how='left')\n","\n","# Merge test with patient_notes on 'pn_num' and 'case_num'\n","test = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n","\n","# Display the first few rows of the merged dataframe\n","display(test.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:44.734072Z","iopub.status.busy":"2022-05-03T13:52:44.733731Z","iopub.status.idle":"2022-05-03T13:52:44.743631Z","shell.execute_reply":"2022-05-03T13:52:44.742809Z","shell.execute_reply.started":"2022-05-03T13:52:44.734034Z"},"trusted":true},"outputs":[],"source":["def prepare_input_fast(cfg, text, feature_text, batch_max_len):\n","    # Tokenize the text and feature text with special tokens, padding, and return offset mappings\n","    inputs = cfg.tokenizer(text, feature_text, \n","                           add_special_tokens=True,\n","                           max_length=batch_max_len,\n","                           padding=\"max_length\",\n","                           return_offsets_mapping=False)\n","    # Convert the tokenized inputs to tensors\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","class TestDatasetFast(Dataset):\n","    # Initialize the dataset with configuration and dataframe\n","    def __init__(self, cfg, df):\n","        # Set the configuration and dataframe\n","        self.cfg = cfg\n","        # Get the feature texts, pn history, and batch max length from the dataframe\n","        self.feature_texts = df['feature_text'].values\n","        # Get the pn history from the dataframe\n","        self.pn_historys = df['pn_history'].values\n","        # Get the batch max length from the dataframe\n","        self.batch_max_len = df['batch_max_length'].values\n","\n","    def __len__(self):\n","        # Return the length of the feature texts\n","        return len(self.feature_texts)\n","\n","    def __getitem__(self, item):\n","        # Prepare the input for the current item\n","        inputs = prepare_input_fast(self.cfg, \n","                                    self.pn_historys[item], \n","                                    self.feature_texts[item],\n","                                    self.batch_max_len[item],\n","                                    )\n","        # Return the prepared input\n","        return inputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:44.745161Z","iopub.status.busy":"2022-05-03T13:52:44.744895Z","iopub.status.idle":"2022-05-03T13:52:44.759816Z","shell.execute_reply":"2022-05-03T13:52:44.759074Z","shell.execute_reply.started":"2022-05-03T13:52:44.745126Z"},"trusted":true},"outputs":[],"source":["class CustomModel(nn.Module):\n","    # Initialize the model with configuration, optional config path, and pretrained flag\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        # Set the configuration\n","        self.cfg = cfg\n","        # If config path is not provided, load the configuration from the model\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","        # Otherwise, load the configuration from the provided path\n","        else:\n","            self.config = torch.load(config_path)\n","        # If pretrained is True, load the model from the pretrained model\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        # Otherwise, load the model from the configuration\n","        else:\n","            self.model = AutoModel.from_config(self.config)\n","        # Add a dropout layer to the model\n","        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n","        # Add a linear layer to the model\n","        self.fc = nn.Linear(self.config.hidden_size, 1)\n","        # Initialize the weights of the linear layer\n","        self._init_weights(self.fc)\n","        \n","    def _init_weights(self, module):\n","        # If the module is a linear layer\n","        if isinstance(module, nn.Linear):\n","            # Normalize the weights with mean 0 and standard deviation from the configuration\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            # If the module has a bias, set it to zero\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        # If the module is an embedding layer\n","        elif isinstance(module, nn.Embedding):\n","            # Normalize the weights with mean 0 and standard deviation from the configuration\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            # If the module has a padding index, set the corresponding weight to zero\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        # If the module is a layer normalization layer\n","        elif isinstance(module, nn.LayerNorm):\n","            # Set the bias to zero\n","            module.bias.data.zero_()\n","            # Set the weight to one\n","            module.weight.data.fill_(1.0)\n","        \n","    def feature(self, inputs):\n","        # Pass the inputs through the model and get the outputs\n","        outputs = self.model(**inputs)\n","        # Get the last hidden states from the outputs\n","        last_hidden_states = outputs[0]\n","        # Return the last hidden states\n","        return last_hidden_states\n","\n","    def forward(self, inputs):\n","        # Pass the inputs through the feature method and get the feature\n","        feature = self.feature(inputs)\n","        # Pass the feature through the dropout layer and the linear layer\n","        output = self.fc(self.fc_dropout(feature))\n","        # Return the output\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:44.762037Z","iopub.status.busy":"2022-05-03T13:52:44.761509Z","iopub.status.idle":"2022-05-03T13:52:44.772272Z","shell.execute_reply":"2022-05-03T13:52:44.771558Z","shell.execute_reply.started":"2022-05-03T13:52:44.761987Z"},"trusted":true},"outputs":[],"source":["def inference_fn_fast(test_loader, model, device):\n","    # Initialize preds as an empty list\n","    preds = []\n","    # Set the model to evaluation mode\n","    model.eval()\n","    # Move the model to the specified device\n","    model.to(device)\n","    # Initialize a progress bar for the test loader\n","    tk0 = tqdm(test_loader, total=len(test_loader))\n","    # Iterate over the test loader\n","    for inputs in tk0:\n","        # for inputs in test_loader\n","        bs = len(inputs['input_ids'])\n","        # Initialize pred_w_pad with zeros\n","        pred_w_pad = np.zeros((bs, CFG.max_len, 1))\n","        # Iterate over the inputs\n","        for k, v in inputs.items():\n","            # Move the input to the specified device\n","            inputs[k] = v.to(device)\n","        # Disable gradient calculation\n","        with torch.no_grad():\n","            # Pass the inputs through the model and get the predictions\n","            y_preds = model(inputs)\n","        # Convert the predictions to a numpy array and apply sigmoid\n","        y_preds = y_preds.sigmoid().to('cpu').numpy()\n","        # Update pred_w_pad with the predictions\n","        pred_w_pad[:, :y_preds.shape[1]] = y_preds\n","        # Append the predictions to the preds list\n","        preds.append(pred_w_pad)\n","    # Concatenate the predictions\n","    predictions = np.concatenate(preds)\n","    # Return the predictions\n","    return predictions"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa-V3-Large-3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:44.775999Z","iopub.status.busy":"2022-05-03T13:52:44.774676Z","iopub.status.idle":"2022-05-03T13:52:44.784562Z","shell.execute_reply":"2022-05-03T13:52:44.783843Z","shell.execute_reply.started":"2022-05-03T13:52:44.775947Z"},"trusted":true},"outputs":[],"source":["# class CFG:\n","#     num_workers=4\n","#     path=\"../input/nbme-deberta-v3-large-3/\"\n","#     config_path=path+'config.pth'\n","#     model=\"microsoft/deberta-v3-large\"\n","#     batch_size=32\n","#     fc_dropout=0.2\n","#     max_len=354\n","#     seed=42\n","#     n_fold=5\n","#     trn_fold=[0, 1, 2, 3, 4]\n","\n","# from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","\n","# tokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\n","# CFG.tokenizer = tokenizer\n","\n","# input_lengths = []\n","# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n","# for text, feature_text in tk0:\n","#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n","#     input_lengths.append(length)\n","# test['input_lengths'] = input_lengths\n","# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n","\n","# # sort dataframe\n","# sort_df = test.iloc[length_sorted_idx]\n","\n","# # calc max_len per batch\n","# sorted_input_length = sort_df['input_lengths'].values\n","# batch_max_length = np.zeros_like(sorted_input_length)\n","# bs = CFG.batch_size\n","# for i in range((len(sorted_input_length)//bs)+1):\n","#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n","# sort_df['batch_max_length'] = batch_max_length\n","\n","# test_dataset = TestDatasetFast(CFG, sort_df)\n","# test_loader = DataLoader(test_dataset,\n","#                       batch_size=CFG.batch_size,\n","#                       shuffle=False,\n","#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","# predictions = []\n","# for fold in CFG.trn_fold:\n","#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n","    \n","#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","#                        map_location=torch.device('cpu'))\n","       \n","#     model.load_state_dict(state['model'])\n","#     prediction = inference_fn_fast(test_loader, model, device)\n","#     prediction = prediction.reshape((len(test), CFG.max_len))\n","    \n","#     ## data re-sort ## \n","#     prediction = prediction[np.argsort(length_sorted_idx)]\n","    \n","#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n","#     predictions.append(char_probs)\n","#     del model, state, prediction, char_probs\n","#     gc.collect()\n","#     torch.cuda.empty_cache()\n","    \n","# predictions_v3_large_1 = np.mean(predictions, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa-V3-Large-6"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:52:44.788285Z","iopub.status.busy":"2022-05-03T13:52:44.788057Z","iopub.status.idle":"2022-05-03T13:55:56.549697Z","shell.execute_reply":"2022-05-03T13:55:56.547997Z","shell.execute_reply.started":"2022-05-03T13:52:44.788253Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    # Configuration for the model\n","    num_workers=4\n","    path=\"../input/nbme-v3-large-6/\"\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    batch_size=32\n","    fc_dropout=0.2\n","    max_len=512\n","    seed=42\n","    n_fold=7\n","    trn_fold=[0, 2, 3, 4, 5, 6, 7]\n","\n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","\n","tokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\n","# Set the tokenizer\n","CFG.tokenizer = tokenizer\n","\n","# Calculate input lengths\n","input_lengths = []\n","# Iterate over the test data\n","tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n","for text, feature_text in tk0:\n","    # Calculate the length of the tokenized input\n","    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n","    # Append the length to the input lengths list\n","    input_lengths.append(length)\n","# Add the input lengths to the test dataframe\n","test['input_lengths'] = input_lengths\n","# Sort the test dataframe by input length\n","length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n","\n","# Sort the test dataframe by input length\n","sort_df = test.iloc[length_sorted_idx]\n","\n","# Calculate the maximum length per batch\n","sorted_input_length = sort_df['input_lengths'].values\n","# Initialize batch_max_length with zeros\n","batch_max_length = np.zeros_like(sorted_input_length)\n","# Set the batch size\n","bs = CFG.batch_size\n","# Calculate the maximum length per batch\n","for i in range((len(sorted_input_length)//bs)+1):\n","    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n","# Add the batch_max_length to the test dataframe\n","sort_df['batch_max_length'] = batch_max_length\n","\n","# Create the test dataset\n","test_dataset = TestDatasetFast(CFG, sort_df)\n","# Create the test loader\n","test_loader = DataLoader(test_dataset,\n","                      batch_size=CFG.batch_size,\n","                      shuffle=False,\n","                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","# Initialize predictions as an empty list\n","predictions = []\n","# Iterate over the training folds\n","for fold in CFG.trn_fold:\n","    # Create the model\n","    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n","    # Load the model state from the specified path\n","    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                       map_location=torch.device('cpu'))\n","    # Load the model state\n","    model.load_state_dict(state['model'])\n","    prediction = inference_fn_fast(test_loader, model, device)\n","    prediction = prediction.reshape((len(test), CFG.max_len))\n","    \n","    # Re-sort the prediction\n","    prediction = prediction[np.argsort(length_sorted_idx)]\n","    \n","    # Get the character probabilities\n","    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n","    # Append the character probabilities to the predictions list\n","    predictions.append(char_probs)\n","    # Delete the model, state, prediction, and character probabilities\n","    del model, state, prediction, char_probs\n","    # Collect garbage\n","    gc.collect()\n","    # Empty the cache\n","    torch.cuda.empty_cache()\n","    \n","# Calculate the mean of the predictions\n","predictions_v3_large_2 = np.mean(predictions, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa-Large"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:55:56.555887Z","iopub.status.busy":"2022-05-03T13:55:56.55402Z","iopub.status.idle":"2022-05-03T13:55:56.56382Z","shell.execute_reply":"2022-05-03T13:55:56.563179Z","shell.execute_reply.started":"2022-05-03T13:55:56.555847Z"},"trusted":true},"outputs":[],"source":["# class CFG:\n","#     num_workers=4\n","#     path=\"../input/nbme-deberta-large-fold-5/\"\n","#     config_path=path+'config.pth'\n","#     model=\"microsoft/deberta-large\"\n","#     batch_size=24\n","#     fc_dropout=0.2\n","#     max_len=466\n","#     seed=42\n","#     n_fold=5\n","#     trn_fold=[0, 1, 2, 3, 4]\n","\n","# CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n","\n","# input_lengths = []\n","# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n","# for text, feature_text in tk0:\n","#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n","#     input_lengths.append(length)\n","# test['input_lengths'] = input_lengths\n","# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n","\n","# # sort dataframe\n","# sort_df = test.iloc[length_sorted_idx]\n","\n","# # calc max_len per batch\n","# sorted_input_length = sort_df['input_lengths'].values\n","# batch_max_length = np.zeros_like(sorted_input_length)\n","# bs = CFG.batch_size\n","# for i in range((len(sorted_input_length)//bs)+1):\n","#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n","# sort_df['batch_max_length'] = batch_max_length\n","\n","# test_dataset = TestDatasetFast(CFG, sort_df)\n","# test_loader = DataLoader(test_dataset,\n","#                       batch_size=CFG.batch_size,\n","#                       shuffle=False,\n","#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","# predictions = []\n","# for fold in CFG.trn_fold:\n","#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n","#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","#                        map_location=torch.device('cpu'))\n","#     model.load_state_dict(state['model'])\n","#     prediction = inference_fn_fast(test_loader, model, device)\n","#     prediction = prediction.reshape((len(test), CFG.max_len))\n","    \n","#     prediction = prediction[np.argsort(length_sorted_idx)]\n","    \n","#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n","#     predictions.append(char_probs)\n","#     del model, state, prediction, char_probs; gc.collect()\n","#     torch.cuda.empty_cache()\n","# predictions_v1_l = np.mean(predictions, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa-Large-4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:55:56.569579Z","iopub.status.busy":"2022-05-03T13:55:56.56756Z","iopub.status.idle":"2022-05-03T13:58:43.513156Z","shell.execute_reply":"2022-05-03T13:58:43.512238Z","shell.execute_reply.started":"2022-05-03T13:55:56.569541Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    # Configuration for the model\n","    num_workers=4\n","    path=\"../input/nbme-large-4/\"\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-large\"\n","    batch_size=24\n","    fc_dropout=0.2\n","    max_len=466\n","    seed=42\n","    n_fold=6\n","    trn_fold=[0, 2, 3, 5, 6, 7]\n","\n","CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path + 'tokenizer/')\n","\n","# Calculate input lengths\n","input_lengths = []\n","# Iterate over the test data\n","tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n","for text, feature_text in tk0:\n","    # Calculate the length of the tokenized input\n","    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n","    # Append the length to the input lengths list\n","    input_lengths.append(length)\n","# Add the input lengths to the test dataframe\n","test['input_lengths'] = input_lengths\n","# Sort the test dataframe by input length\n","length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n","\n","# Sort the test dataframe by input length\n","sort_df = test.iloc[length_sorted_idx]\n","\n","# Calculate the maximum length per batch\n","sorted_input_length = sort_df['input_lengths'].values\n","# Initialize batch_max_length with zeros\n","batch_max_length = np.zeros_like(sorted_input_length)\n","# Set the batch size\n","bs = CFG.batch_size\n","# Calculate the maximum length per batch\n","for i in range((len(sorted_input_length)//bs)+1):\n","    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n","# Add the batch_max_length to the test dataframe\n","sort_df['batch_max_length'] = batch_max_length\n","\n","# Create the test dataset\n","test_dataset = TestDatasetFast(CFG, sort_df)\n","# Create the test loader\n","test_loader = DataLoader(test_dataset,\n","                      batch_size=CFG.batch_size,\n","                      shuffle=False,\n","                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","predictions = []\n","# Iterate over the training folds\n","for fold in CFG.trn_fold:\n","    # Create the model\n","    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n","    # Load the model state from the specified path\n","    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                       map_location=torch.device('cpu'))\n","    # Load the model state\n","    model.load_state_dict(state['model'])\n","    # Get the predictions\n","    prediction = inference_fn_fast(test_loader, model, device)\n","    # Reshape the predictions\n","    prediction = prediction.reshape((len(test), CFG.max_len))\n","    # Re-sort the prediction\n","    prediction = prediction[np.argsort(length_sorted_idx)]\n","    # Get the character probabilities\n","    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n","    # Append the character probabilities to the predictions list\n","    predictions.append(char_probs)\n","    # Delete the model, state, prediction, and character probabilities\n","    del model, state, prediction, char_probs; gc.collect()\n","    # Empty the cache\n","    torch.cuda.empty_cache()\n","# Calculate the mean of the predictions\n","predictions_v1_2 = np.mean(predictions, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa-Large-MNLI"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:58:43.519497Z","iopub.status.busy":"2022-05-03T13:58:43.517618Z","iopub.status.idle":"2022-05-03T13:58:43.527604Z","shell.execute_reply":"2022-05-03T13:58:43.52681Z","shell.execute_reply.started":"2022-05-03T13:58:43.519452Z"},"trusted":true},"outputs":[],"source":["# class CFG:\n","#     num_workers=4\n","#     path=\"../input/nbme-deberta-large-mnli/\"\n","#     config_path=path+'config.pth'\n","#     model=\"microsoft/deberta-large-mnli\"\n","#     batch_size=24\n","#     fc_dropout=0.2\n","#     max_len=466\n","#     seed=1024\n","#     n_fold=5\n","#     trn_fold=[0, 1, 2, 3, 4]\n","\n","# CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n","\n","# input_lengths = []\n","# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n","# for text, feature_text in tk0:\n","#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n","#     input_lengths.append(length)\n","# test['input_lengths'] = input_lengths\n","# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n","\n","# # sort dataframe\n","# sort_df = test.iloc[length_sorted_idx]\n","\n","# # calc max_len per batch\n","# sorted_input_length = sort_df['input_lengths'].values\n","# batch_max_length = np.zeros_like(sorted_input_length)\n","# bs = CFG.batch_size\n","# for i in range((len(sorted_input_length)//bs)+1):\n","#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n","# sort_df['batch_max_length'] = batch_max_length\n","\n","# test_dataset = TestDatasetFast(CFG, sort_df)\n","# test_loader = DataLoader(test_dataset,\n","#                       batch_size=CFG.batch_size,\n","#                       shuffle=False,\n","#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","# predictions = []\n","# for fold in CFG.trn_fold:\n","#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n","#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","#                        map_location=torch.device('cpu'))\n","#     model.load_state_dict(state['model'])\n","#     prediction = inference_fn_fast(test_loader, model, device)\n","#     prediction = prediction.reshape((len(test), CFG.max_len))\n","    \n","#     prediction = prediction[np.argsort(length_sorted_idx)]\n","    \n","#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n","#     predictions.append(char_probs)\n","#     del model, state, prediction, char_probs; gc.collect()\n","#     torch.cuda.empty_cache()\n","# predictions_large_mnli_1 = np.mean(predictions, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa-Large-MNLI-3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T13:58:43.533289Z","iopub.status.busy":"2022-05-03T13:58:43.531502Z","iopub.status.idle":"2022-05-03T14:01:55.636168Z","shell.execute_reply":"2022-05-03T14:01:55.635324Z","shell.execute_reply.started":"2022-05-03T13:58:43.533254Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    # Configuration for the model\n","    num_workers=4\n","    path=\"../input/nbme-large-mnli-3/\"\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-large-mnli\"\n","    batch_size=24\n","    fc_dropout=0.2\n","    max_len=466\n","    seed=1024\n","    n_fold=7\n","    trn_fold=[0, 2, 3, 4, 5, 6, 7]\n","\n","CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path + 'tokenizer/')\n","\n","# Calculate input lengths\n","input_lengths = []\n","# Iterate over the test data\n","tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n","for text, feature_text in tk0:\n","    # Calculate the length of the tokenized input\n","    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n","    # Append the length to the input lengths list\n","    input_lengths.append(length)\n","# Add the input lengths to the test dataframe\n","test['input_lengths'] = input_lengths\n","# Sort the test dataframe by input length\n","length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n","\n","# Sort the test dataframe by input length\n","sort_df = test.iloc[length_sorted_idx]\n","\n","# Calculate the maximum length per batch\n","sorted_input_length = sort_df['input_lengths'].values\n","# Initialize batch_max_length with zeros\n","batch_max_length = np.zeros_like(sorted_input_length)\n","# Recalculate the maximum length per batch\n","bs = CFG.batch_size\n","for i in range((len(sorted_input_length)//bs)+1):\n","    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n","# Add the batch_max_length to the test dataframe\n","sort_df['batch_max_length'] = batch_max_length\n","\n","# Create the test dataset\n","test_dataset = TestDatasetFast(CFG, sort_df)\n","# Create the test loader\n","test_loader = DataLoader(test_dataset,\n","                      batch_size=CFG.batch_size,\n","                      shuffle=False,\n","                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","predictions = []\n","# Iterate over the training folds\n","for fold in CFG.trn_fold:\n","    # Create the model\n","    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n","    # Load the model state from the specified path\n","    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                       map_location=torch.device('cpu'))\n","    # Load the model state\n","    model.load_state_dict(state['model'])\n","    # Get the predictions\n","    prediction = inference_fn_fast(test_loader, model, device)\n","    # Reshape the predictions\n","    prediction = prediction.reshape((len(test), CFG.max_len))\n","    # Re-sort the prediction\n","    prediction = prediction[np.argsort(length_sorted_idx)]\n","    # Get the character probabilities\n","    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n","    # Append the character probabilities to the predictions list\n","    predictions.append(char_probs)\n","    # Delete the model, state, prediction, and character probabilities\n","    del model, state, prediction, char_probs; gc.collect()\n","    # Empty the cache\n","    torch.cuda.empty_cache()\n","# Calculate the mean of the predictions\n","predictions_large_mnli_2 = np.mean(predictions, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa-XLarge"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T14:01:55.64282Z","iopub.status.busy":"2022-05-03T14:01:55.640911Z","iopub.status.idle":"2022-05-03T14:06:13.941286Z","shell.execute_reply":"2022-05-03T14:06:13.940499Z","shell.execute_reply.started":"2022-05-03T14:01:55.642774Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    # Configuration for the model\n","    num_workers=4\n","    path=\"../input/nbme-xlarge-3/\"\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-xlarge\"\n","    batch_size=24\n","    fc_dropout=0.2\n","    max_len=466\n","    seed=42\n","    n_fold=6\n","    trn_fold=[0, 2, 3, 5, 6, 7]\n","\n","CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path + 'tokenizer/')\n","\n","# Calculate input lengths\n","input_lengths = []\n","# Iterate over the test data\n","tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n","for text, feature_text in tk0:\n","    # Calculate the length of the tokenized input\n","    length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n","    # Append the length to the input lengths list\n","    input_lengths.append(length)\n","# Add the input lengths to the test dataframe\n","test['input_lengths'] = input_lengths\n","# Sort the test dataframe by input length\n","length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n","\n","# sort dataframe\n","sort_df = test.iloc[length_sorted_idx]\n","\n","# Calculate the maximum length per batch\n","sorted_input_length = sort_df['input_lengths'].values\n","# Initialize batch_max_length with zeros\n","batch_max_length = np.zeros_like(sorted_input_length)\n","# Set the batch size\n","bs = CFG.batch_size\n","# Calculate the maximum length per batch\n","for i in range((len(sorted_input_length)//bs)+1):\n","    batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n","# Add the batch_max_length to the test dataframe\n","sort_df['batch_max_length'] = batch_max_length\n","\n","# Create the test dataset\n","test_dataset = TestDatasetFast(CFG, sort_df)\n","# Create the test loader\n","test_loader = DataLoader(test_dataset,\n","                      batch_size=CFG.batch_size,\n","                      shuffle=False,\n","                      num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","predictions = []\n","# Iterate over the training folds\n","for fold in CFG.trn_fold:\n","    # Create the model\n","    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n","    # Load the model state from the specified path\n","    state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                       map_location=torch.device('cpu'))\n","    # Load the model state\n","    model.load_state_dict(state['model'])\n","    # Get the predictions\n","    prediction = inference_fn_fast(test_loader, model, device)\n","    # Reshape the predictions\n","    prediction = prediction.reshape((len(test), CFG.max_len))\n","    # Re-sort the prediction\n","    prediction = prediction[np.argsort(length_sorted_idx)]\n","    # Get the character probabilities\n","    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n","    # Append the character probabilities to the predictions list\n","    predictions.append(char_probs)\n","    # Delete the model, state, prediction, and character probabilities\n","    del model, state, prediction, char_probs; gc.collect()\n","    # Empty the cache\n","    torch.cuda.empty_cache()\n","# Calculate the mean of the predictions\n","predictions_xl = np.mean(predictions, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## DeBERTa-Base"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T14:06:13.947427Z","iopub.status.busy":"2022-05-03T14:06:13.945542Z","iopub.status.idle":"2022-05-03T14:06:13.955395Z","shell.execute_reply":"2022-05-03T14:06:13.954539Z","shell.execute_reply.started":"2022-05-03T14:06:13.947385Z"},"trusted":true},"outputs":[],"source":["# # ====================================================\n","# # CFG\n","# # ====================================================\n","# class CFG:\n","#     num_workers=4\n","#     path=\"../input/nbme-deberta-base/\"\n","#     config_path=path+'config.pth'\n","#     model=\"microsoft/deberta-base\"\n","#     batch_size=24\n","#     fc_dropout=0.2\n","#     max_len=466\n","#     seed=42\n","#     n_fold=5\n","#     trn_fold=[0, 1, 2, 3, 4]\n","\n","# CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n","\n","# input_lengths = []\n","# tk0 = tqdm(zip(test['pn_history'].fillna(\"\").values, test['feature_text'].fillna(\"\").values), total=len(test))\n","# for text, feature_text in tk0:\n","#     length = len(CFG.tokenizer(text, feature_text, add_special_tokens=True)['input_ids'])\n","#     input_lengths.append(length)\n","# test['input_lengths'] = input_lengths\n","# length_sorted_idx = np.argsort([-len_ for len_ in input_lengths])\n","\n","# # sort dataframe\n","# sort_df = test.iloc[length_sorted_idx]\n","\n","# # calc max_len per batch\n","# sorted_input_length = sort_df['input_lengths'].values\n","# batch_max_length = np.zeros_like(sorted_input_length)\n","# bs = CFG.batch_size\n","# for i in range((len(sorted_input_length)//bs)+1):\n","#     batch_max_length[i*bs:(i+1)*bs] = np.max(sorted_input_length[i*bs:(i+1)*bs])    \n","# sort_df['batch_max_length'] = batch_max_length\n","\n","# test_dataset = TestDatasetFast(CFG, sort_df)\n","# test_loader = DataLoader(test_dataset,\n","#                       batch_size=CFG.batch_size,\n","#                       shuffle=False,\n","#                       num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","# predictions = []\n","# for fold in CFG.trn_fold:\n","#     model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n","#     state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","#                        map_location=torch.device('cpu'))\n","#     model.load_state_dict(state['model'])\n","#     prediction = inference_fn_fast(test_loader, model, device)\n","#     prediction = prediction.reshape((len(test), CFG.max_len))\n","#     ## data re-sort ## \n","#     prediction = prediction[np.argsort(length_sorted_idx)]\n","\n","#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n","#     predictions.append(char_probs)\n","#     del model, state, prediction, char_probs; gc.collect()\n","#     torch.cuda.empty_cache()\n","# predictions_b_1 = np.mean(predictions, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T14:06:13.961254Z","iopub.status.busy":"2022-05-03T14:06:13.959143Z","iopub.status.idle":"2022-05-03T14:06:13.971637Z","shell.execute_reply":"2022-05-03T14:06:13.971018Z","shell.execute_reply.started":"2022-05-03T14:06:13.961215Z"},"trusted":true},"outputs":[],"source":["# 0.55-0.40-0.10-0.15\n","w1 = 0.35     # v3 large    8825, 882\n","w2 = 0.25     # large       8783, 880\n","w3 = 0.35     # large-mnli  8795, 882\n","w4 = 0.25     # xlarge      8785, 882\n","\n","# predictions_v3_large_2      0.8826\t0.882\n","# predictions_v1_l            0.8783\t0.88\n","# predictions_v1_2            0.8789\t0.883\n","# predictions_large_mnli_2    0.8806\t0.882\n","# predictions_xl              0.8785\t0.882\n","# predictions_b_1             \n","\n","# w1, w2, w3, w4 = 0.55, 0.15, 0.35, 0.15              0.886\n","# w1, w2, w3, w4, w5 = 0.55, 0.15, 0.25, 0.15, 0.1     0.885\n","# w1, w2, w3, w4, w5 = 0.45, 0.15, 0.35, 0.15, 0.1     0.885\n","# w1, w2, w3, w4, w5 = 0.45, 0.15, 0.35, 0.15, 0.05    0.886\n","# w1, w2, w3, w4, w5 = 0.55, 0.10, 0.40, 0.10, 0.05    0.885\n","# w1, w2, w3, w4 = 0.65, 0.15, 0.25, 0.15              0.886\n","# w1, w2, w3, w4 = 0.65, 0.10, 0.35, 0.10              0.885\n","# w1, w2, w3, w4 = 0.75, 0.10, 0.25, 0.10              0.885\n","# w1, w2, w3, w4 = 0.55, 0.15, 0.35, 0.15              0.886\n","# w1, w2, w3, w4 = 0.65, 0.15, 0.25, 0.15              0.886\n","# w1, w2, w3, w4 = 0.65, 0.15, 0.15, 0.10              0.885\n","\n","# w1, w2, w3 = 0.55, 0.35, 0.20                        0.887\n","\n","# 0.886\n","# w1, w2, w3, w4 = 0.55, 0.25, 0.15, 0.10\n","\n","# 0.887\n","w1, w2, w3, w4 = 0.55, 0.20, 0.15, 0.10\n","\n","\n","\n","predictions = []\n","# Ensemble the predictions\n","for p1, p2, p3, p4 in zip(predictions_v3_large_2, predictions_xl, predictions_large_mnli_2, predictions_v1_2):\n","    predictions.append(w1 * p1 + w2 * p2 + w3 * p3 + w4 * p4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T14:06:13.977988Z","iopub.status.busy":"2022-05-03T14:06:13.975717Z","iopub.status.idle":"2022-05-03T14:06:14.002742Z","shell.execute_reply":"2022-05-03T14:06:14.00209Z","shell.execute_reply.started":"2022-05-03T14:06:13.977951Z"},"trusted":true},"outputs":[],"source":["# Get the results\n","results = get_results(predictions)\n","# Add the results to the submission dataframe\n","submission['location'] = results\n","# Display the submission dataframe\n","display(submission.head())\n","# Save the submission dataframe to a CSV file\n","submission[['id', 'location']].to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":3075283,"sourceId":33607,"sourceType":"competition"},{"datasetId":1913766,"sourceId":3161854,"sourceType":"datasetVersion"},{"datasetId":1966628,"sourceId":3244848,"sourceType":"datasetVersion"},{"datasetId":2063895,"sourceId":3424515,"sourceType":"datasetVersion"},{"datasetId":2120421,"sourceId":3524426,"sourceType":"datasetVersion"},{"datasetId":2121324,"sourceId":3525880,"sourceType":"datasetVersion"},{"datasetId":2124005,"sourceId":3531020,"sourceType":"datasetVersion"},{"datasetId":2127117,"sourceId":3536818,"sourceType":"datasetVersion"},{"datasetId":2130728,"sourceId":3543182,"sourceType":"datasetVersion"},{"datasetId":2132669,"sourceId":3546761,"sourceType":"datasetVersion"},{"datasetId":2132674,"sourceId":3546774,"sourceType":"datasetVersion"},{"datasetId":2132706,"sourceId":3546840,"sourceType":"datasetVersion"},{"datasetId":2140419,"sourceId":3561446,"sourceType":"datasetVersion"},{"datasetId":2140472,"sourceId":3561566,"sourceType":"datasetVersion"},{"datasetId":2142078,"sourceId":3564538,"sourceType":"datasetVersion"},{"datasetId":2143812,"sourceId":3567916,"sourceType":"datasetVersion"}],"dockerImageVersionId":30177,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
